---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'PeruSIL: A Framework to Build a Continuous Peruvian Sign Language Interpretation
  Dataset'
subtitle: ''
summary: ''
authors:
- Gissella Bejarano
- Joe Huamani-Malca
- Francisco Cerna-Herrera
- alvamanchegof
- Pablo Rivas
tags: []
categories: []
date: '2022-06-01'
lastmod: 2023-02-14T14:22:53Z
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-02-14T14:22:53.052484Z'
publication_types:
- '1'
abstract: 'Video-based datasets for Continuous Sign Language are scarce due to the
  challenging task of recording videos from native signers and the reduced number
  of people who can annotate sign language. COVID-19 has evidenced the key role of
  sign language interpreters in delivering nationwide health messages to deaf communities.
  In this paper, we present a framework for creating a multi-modal sign language interpretation
  dataset based on videos and we use it to create the first dataset for Peruvian Sign
  Language (LSP) interpretation annotated by hearing volunteers who have intermediate
  knowledge of PSL guided by the video audio. We rely on hearing people to produce
  a first version of the annotations, which should be reviewed by native signers in
  the future. Our contributions: i) we design a framework to annotate a sign Language
  dataset; ii) we release the first annotated LSP multi-modal interpretation dataset
  (AEC); iii) we evaluate the annotation done by hearing people by training a sign
  language recognition model. Our model reaches up to 80.3% of accuracy among a minimum
  of five classes (signs) AEC dataset, and 52.4% in a second dataset. Nevertheless,
  analysis by subject in the second dataset show variations worth to discuss.'
publication: '*Proceedings of the LREC2022 10th Workshop on the Representation and
  Processing of Sign Languages: Multilingual Sign Language Resources*'
publication_short: '*SignLang 2022*'
url_pdf: https://aclanthology.org/2022.signlang-1.1.pdf
---
