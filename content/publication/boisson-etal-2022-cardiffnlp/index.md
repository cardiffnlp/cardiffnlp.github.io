---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'CardiffNLP-Metaphor at SemEval-2022 Task 2: Targeted Fine-tuning of Transformer-based
  Language Models for Idiomaticity Detection'
subtitle: ''
summary: ''
authors:
- boissonj
- camachocolladosj
- espinosaankel
tags: []
categories: []
date: '2022-07-01'
lastmod: 2023-02-14T15:35:05Z
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-02-14T15:35:04.985379Z'
publication_types:
- '1'
abstract: This paper describes the experiments ran for SemEval-2022 Task 2, subtask
  A, zero-shot and one-shot settings for idiomaticity detection. Our main approach
  is based on fine-tuning transformer-based language models as a baseline to perform
  binary classification. Our system, CardiffNLP-Metaphor, ranked 8th and 7th (respectively
  on zero- and one-shot settings on this task. Our main contribution lies in the extensive
  evaluation of transformer-based language models and various configurations, showing,
  among others, the potential of large multilingual models over base monolingual models.
  Moreover, we analyse the impact of various input parameters, which offer interesting
  insights on how language models work in practice.
publication: '*Proceedings of the 16th International Workshop on Semantic Evaluation
  (SemEval-2022)*'
publication_short: '*SemEval-2022*'
doi: 10.18653/v1/2022.semeval-1.20
url_pdf: https://aclanthology.org/2022.semeval-1.20.pdf
---
