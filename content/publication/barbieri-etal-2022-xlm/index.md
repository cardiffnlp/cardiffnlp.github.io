---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and
  Beyond'
subtitle: ''
summary: ''
authors:
- Francesco Barbieri
- espinosaankel
- camachocolladosj
tags: []
categories: []
date: '2022-06-01'
lastmod: 2023-02-14T15:35:03Z
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-02-14T15:35:02.881403Z'
publication_types:
- '1'
abstract: 'Language models are ubiquitous in current NLP, and their multilingual capacity
  has recently attracted considerable attention. However, current analyses have almost
  exclusively focused on (multilingual variants of) standard benchmarks, and have
  relied on clean pre-training and task-specific corpora as multilingual signals.
  In this paper, we introduce XLM-T, a model to train and evaluate multilingual language
  models in Twitter. In this paper we provide: (1) a new strong multilingual baseline
  consisting of an XLM-R (Conneau et al. 2020) model pre-trained on millions of tweets
  in over thirty languages, alongside starter code to subsequently fine-tune on a
  target task; and (2) a set of unified sentiment analysis Twitter datasets in eight
  different languages and a XLM-T model trained on this dataset.'
publication: '*Proceedings of the Thirteenth Language Resources and Evaluation Conference*'
publication_short: '*LREC 2022*'
url_pdf: https://aclanthology.org/2022.lrec-1.27.pdf
---
